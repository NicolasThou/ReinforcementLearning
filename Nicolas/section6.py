import numpy as np
from matplotlib import pyplot as plt
import random
"""
1 - Sample trajectories from a random uniform policy with a finite, large enough time horizon T. Implement a routine 
which iteratively updates Qb(xk , uk ) with Q-learning from these trajectories. Run your routine with a constant 
learning rate α = 0.05. Derive directly µb∗ from Qb. Display JN µb∗ , along with JN µ∗ , for each initial state x.

"""

def q_table():
    """
    
    """

def offline_q_learning(trajectory):
    """
    Compute iteratively Q(xk, uk) with α = 0.05.

    Argument:
    =========
    trajectory : is the historic of the agent (x(t), u(t), r(t), x(t+1) )

    Return:
    ======
    Return a value
    """




if __name__ == '__main__':
    """
      

    """






